llm_config = {
    "config_list": [
        {
            "model": "llama3.1:8b",
            "base_url": "http://localhost:11434/v1",
            "api_key": "ollama",
            "price": [0.0, 0.0],
        }
    ],
    "timeout": 300,
    "temperature": 0.1,
    "max_tokens": 1024,
}